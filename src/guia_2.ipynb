{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from faker import Faker\n",
    "random.seed(49)\n",
    "fake = Faker(seed=49,locale=\"es\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formato_fecha = \"%d-%m-%Y\"\n",
    "\n",
    "def str_a_date(fecha_str):\n",
    "    d,m,y = fecha_str.split(\"-\")\n",
    "    return datetime.date(int(y), int(m), int(d))\n",
    "\n",
    "def es_fecha_ultimos_anios(fecha, anios=2):\n",
    "    hoy = datetime.datetime.today()\n",
    "    fecha = str_a_date(fecha)\n",
    "    anios_desde_fecha = relativedelta(hoy,fecha).years\n",
    "    return anios_desde_fecha < anios\n",
    "\n",
    "def get_fecha_ultimos_anios(anios=3, formato=formato_fecha): \n",
    "    return fake.date_between(\"-{}y\".format(anios)).strftime(formato)\n",
    "\n",
    "def get_precio(fecha, producto):\n",
    "    \"\"\"\n",
    "        En base a una fecha y un codigo de producto, devuelve un precio simulando una inflacion random.\n",
    "    \"\"\"\n",
    "    fecha_int = int((str_a_date(fecha)-datetime.date(2000,1,1)).total_seconds() / 1000000)\n",
    "    return fecha_int * int(producto)\n",
    "\n",
    "year = lambda x: x.split(\"-\")[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1- Se tiene un RDD con el registro de notas de los alumnos de la forma (padrón, materia, nota,fecha). Se pide resolver utilizando PySpark:\n",
    "\n",
    "A. Cuántos alumnos aprobaron al menos 1 materia en los últimos 2 años.\n",
    "\n",
    "B. Un RDD conteniendo el promedio de notas de cada alumno de la forma (padrón,promedio).\n",
    "\n",
    "C. El nombre y apellido del alumno con mejor promedio. Para esto puede utilizarse un segundo RDD alumnos con registros (padron, nombre y apellido)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cantidad_alumnos = 20\n",
    "padrones = [ x for x in range(1, cantidad_alumnos+1) ]\n",
    "materias = (\"ALGEBRA I\", \"ANALISIS I\", \"ALGEBRA II\", \"ANALISIS II\", \"PROBABILIDAD\", \"ALGORITMOS I\", \"ALGORITMOS II\")\n",
    "notas = list(range(1,11))\n",
    "fechas = (\"01-02-2019\", \"01-07-2019\", \"01-02-2018\", \"01-07-2018\", \"01-12-2018\", \"01-02-2017\", \"01-07-2017\", \"01-12-2017\")\n",
    "def alumno_random(): return tuple([random.choice(x) for x in (padrones, materias, notas, fechas)])\n",
    "\n",
    "cantidad_registros = 50\n",
    "notas = { alumno_random() for a in range(cantidad_registros) }\n",
    "\n",
    "rdd_notas = sc.parallelize(notas)\n",
    "\n",
    "es_aprobado = lambda x: x > 3\n",
    "\n",
    "ejercicio_a = rdd_notas\\\n",
    "    .filter(lambda x: es_aprobado(x[2]))\\\n",
    "    .filter(lambda x: es_fecha_ultimos_anios(x[3]))\\\n",
    "    .map(lambda x: (x[0], 0))\\\n",
    "    .groupByKey()\\\n",
    "    .count()\n",
    "\n",
    "print(\"{} alumnos de {} aprobaron al menos 1 materia en los últimos 2 años\".format(ejercicio_a, cantidad_alumnos))\n",
    "\n",
    "ejercicio_b = rdd_notas\\\n",
    "    .map(lambda x: (x[0], (x[2], 1)))\\\n",
    "    .reduceByKey(lambda x,y: (x[0]+y[0],x[1]+1))\\\n",
    "    .map(lambda x: (x[0], x[1][0]/x[1][1]))\n",
    "# pide el RDD, no ejecuto ninguna acción\n",
    "print(ejercicio_b)\n",
    "\n",
    "nombres = [(x, fake.first_name(),fake.last_name()) for x in padrones]\n",
    "rdd_nombres = sc.parallelize(nombres)\n",
    "\n",
    "padron_mejor_promedio = ejercicio_b.max(lambda x: x[1])[0]\n",
    "ejercicio_c = rdd_nombres.filter(lambda x: x[0]==padron_mejor_promedio).collect()[0]\n",
    "print(\"'{}, {}' (padron {}) es el alumno con mejor promedio\".format(ejercicio_c[2], ejercicio_c[1], ejercicio_c[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "2- Se tiene un RDD registros de ventas de producto con la forma (fecha de venta, código de  producto, precio de venta) y en otro RDD detalle de los productos con (código de producto, descripción del producto, categoría). Se pide resolver utilizando PySpark:\n",
    "\n",
    "A. Cuál es el producto más vendido.\n",
    "\n",
    "B. Cuál es la categoría de productos más vendida.\n",
    "\n",
    "C. Cuál es el top5 de productos más vendidos generando un RDD con (código de producto, descripción, cantidad de ventas)\n",
    "\n",
    "D. Cuál es el producto que registró mayor aumento de precio en el último año, tomando para este análisis solo los productos que cuenten con al menos 50 ventas en el último año.\n",
    "\n",
    "E. Idem anterior, pero calculando la categoría de productos que registró mayor variación de precios en el último año."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cantidad_ventas = 20000\n",
    "ids_productos = list(range(1, 11))\n",
    "categorias = [\"Autos\", \"Motos\", \"Lanchas\", \"Aviones\"]\n",
    "productos = [ (a,fake.street_name(),random.choice(categorias)) for a in ids_productos ]\n",
    "\n",
    "def crear_venta():\n",
    "    fecha, producto = get_fecha_ultimos_anios(), random.choice(ids_productos)\n",
    "    return (fecha, producto, get_precio(fecha, producto))\n",
    "\n",
    "ventas = [crear_venta() for x in range(cantidad_ventas)]\n",
    "rdd_ventas = sc.parallelize(ventas)\n",
    "rdd_productos = sc.parallelize(productos)\n",
    "\n",
    "# A. Cuál es el producto más vendido.\n",
    "codigo_mas_vendido = rdd_ventas.map(lambda x: (x[1], 0))\\\n",
    "    .reduceByKey(lambda x,y: x+1)\\\n",
    "    .max(lambda x: x[1])[0]\n",
    "producto_mas_vendido = rdd_productos.filter(lambda x: x[0]==codigo_mas_vendido).collect()[0]\n",
    "print(\"A) El producto más vendido es {}\".format(producto_mas_vendido))\n",
    "\n",
    "# B. Cuál es la categoría de productos más vendida.\n",
    "categoria_mas_vendida = rdd_ventas.map(lambda x: (x[1], 0))\\\n",
    "    .join(rdd_productos.map(lambda x: (x[0], x[2])))\\\n",
    "    .map(lambda x: (x[1][1],0))\\\n",
    "    .reduceByKey(lambda x,y: x+1)\\\n",
    "    .max(lambda x: x[1])\n",
    "print(\"B) La categoria mas vendida es {}, con {} ventas\".format(*categoria_mas_vendida))\n",
    "\n",
    "# C. Cuál es el top5 de productos más vendidos generando un RDD con (código de producto, descripción, cantidad de ventas)\n",
    "top_5 = rdd_ventas.map(lambda x: (x[1], 0))\\\n",
    "    .reduceByKey(lambda x,y: x+1)\\\n",
    "    .top(5, lambda x: x[1])\n",
    "top_5 = sc.parallelize(top_5).join(rdd_productos.map(lambda x: (x[0],x[1]))).map(lambda x: (x[0], *x[1])).collect()\n",
    "print(\"C)\", sorted(top_5, key=lambda x: x[1], reverse=True))\n",
    "\n",
    "# D. Cuál es el producto que registró mayor aumento de precio en el último año, tomando para este análisis solo los productos que cuenten con al menos 50 ventas en el último año.\n",
    "# Voy a considerar promedio año actual vs promedio año anterior\n",
    "\n",
    "ventas_ultimo_anio = rdd_ventas.filter(lambda x: es_fecha_ultimos_anios(x[0], 1)) #(fecha de venta, código de producto, precio de venta)\n",
    "\n",
    "def min_max_ventas(x, y):\n",
    "    \"\"\" no fue magia \"\"\"\n",
    "    fecha_min_x, venta_min_x, fecha_max_x, venta_max_x, cant_x = x\n",
    "    fecha_min_y, venta_min_y, fecha_max_y, venta_max_y, cant_y = y\n",
    "\n",
    "    min_x = str_a_date(fecha_min_x) < str_a_date(fecha_min_y)\n",
    "    fecha_min = fecha_min_x if min_x else fecha_min_y\n",
    "    venta_min = venta_min_x if min_x else venta_min_y\n",
    "    \n",
    "    max_x = str_a_date(fecha_max_x) > str_a_date(fecha_max_y)\n",
    "    fecha_max = fecha_max_x if max_x else fecha_max_y\n",
    "    venta_max = venta_max_x if max_x else venta_max_y\n",
    "        \n",
    "    return (fecha_min, venta_min, fecha_max, venta_max, cant_x+1)\n",
    "\n",
    "codigo, aumento = ventas_ultimo_anio.map(lambda x: (x[1], (x[0],x[2],x[0],x[2],0)))\\\n",
    "    .reduceByKey(min_max_ventas)\\\n",
    "    .filter(lambda x: int(x[1][-1]) > 50)\\\n",
    "    .map(lambda x: (x[0],int(x[1][3])-int(x[1][1])))\\\n",
    "    .max(lambda x: x[1])\n",
    "producto = rdd_productos.filter(lambda x: x[0]==codigo).collect()[0]\n",
    "print(\"D) El producto {} registró un aumento de {} el último año\".format(producto, aumento))\n",
    "\n",
    "# E. Cuál es la categoria que registró mayor aumento de precio en el último año.\n",
    "# Voy a considerar promedio año actual vs promedio año anterior\n",
    "\n",
    "este_anio = datetime.date.today().year\n",
    "anios = { str(este_anio-1), str(este_anio) }\n",
    "\n",
    "actual = lambda x:  x==str(este_anio)\n",
    "\n",
    "def mapper(x):\n",
    "    \"\"\"\n",
    "        (cod, ((anio,precio),categoria)) -> (categoria, (suma_actual, suma_anterior, cant_actual, cant_anterior))\n",
    "    \"\"\"\n",
    "    \n",
    "    cod, ((anio, precio), categoria) = x\n",
    "    es_actual = actual(anio)\n",
    "    return (categoria, (precio if es_actual else 0, precio if not es_actual else 0, int(es_actual), int(not es_actual)))\n",
    "\n",
    "def reducer(x,y):\n",
    "    return tuple([sum(a) for a in zip(x,y)])\n",
    "\n",
    "def map_dif(x):\n",
    "    \"\"\"\n",
    "        (categoria, (suma_actual, suma_anterior, cant_actual, cant_anterior)) -> (categoria, aumento)\n",
    "    \"\"\"\n",
    "    categoria, (suma_actual, suma_anterior, cant_actual, cant_anterior) = x\n",
    "    avg_actual, avg_anterior = suma_actual/cant_actual , suma_anterior/cant_anterior\n",
    "    aumento = (avg_actual/avg_anterior - 1) * 100\n",
    "    return categoria, round(aumento,2)\n",
    "\n",
    "aumentos = rdd_ventas.filter(lambda x: year(x[0]) in anios) \\\n",
    "    .map(lambda x: (x[1], (year(x[0]), x[2])))\\\n",
    "    .join(rdd_productos.map(lambda x: (x[0], x[2])))\\\n",
    "    .map(mapper)\\\n",
    "    .reduceByKey(reducer)\\\n",
    "    .map(map_dif)\n",
    "\n",
    "max_aumento = aumentos.max(lambda x: x[1])\n",
    "\n",
    "print(\"E) La categoria con más aumento ({}%) es '{}'\".format(max_aumento[1], max_aumento[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "4- Se tiene un RDD con las coordenadas de rectángulos de la forma (x1,x2,y1,y2).\n",
    "\n",
    "Se pide programar en PySpark un programa que encuentre el rectángulo de superficie mínima que contiene al punto (w,z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rectangulo_random = lambda: tuple((random.randint(-100,100) for a in range(4)))\n",
    "cant_rectangulos = 200\n",
    "rdd_rectangulos = sc.parallelize([rectangulo_random() for x in range(cant_rectangulos)])\n",
    "\n",
    "superficie_rectangulo = lambda x: abs(x[3]-x[2])*abs(x[1]-x[0])\n",
    "def contiene(rectangulo, punto) -> bool:\n",
    "    \"\"\"\n",
    "        rectangulo: (x1,x2,y1,y2)\n",
    "        punto: (x,y)\n",
    "        ! todos enteros\n",
    "    \"\"\"\n",
    "    x, y = punto\n",
    "    min_x, max_x = sorted(rectangulo[:2])\n",
    "    min_y, max_y = sorted(rectangulo[2:])\n",
    "    return (min_x <= x <= max_x) and (min_y <= y <= max_y)\n",
    "\n",
    "def encontrar_minimo(punto, rectangulos):\n",
    "    \"\"\"\n",
    "        punto: (x,y)\n",
    "        rectangulos: RDD [(x1,x2,y1,y2)]\n",
    "    \"\"\"\n",
    "    minimo = rectangulos.filter(lambda x: contiene(x, punto))\\\n",
    "        .map(lambda x: (*x, superficie_rectangulo(x)))\\\n",
    "        .min(lambda x: x[-1])\n",
    "    return minimo[:-1]\n",
    "\n",
    "encontrar_minimo((3,6), rdd_rectangulos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "5- Se tiene un RDD con libros en donde cada registro es un texto. Se pide obtener todos los anagramas de mas de 7 letras que puedan encontrarse. El formato de salida debe ser una lista\n",
    "de listas en donde cada lista tiene un conjunto de palabras que son anagramas. \n",
    "\n",
    "Ejemplo: [[discounter,introduces,reductions],[percussion,supersonic]...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "Quise hacer un fake para probar pero no da ni un anagrama de resultado:\n",
    "# cant_textos, cant_caracteres = 100, 200000\n",
    "# rdd_textos = sc.parallelize(fake.texts(cant_textos, cant_caracteres))\n",
    "\n",
    "\"\"\"\n",
    "rdd_textos = sc.parallelize([\"TEXTOTEXTUAL OTROTEXTO pruebita\", \"TEXTOTEXTUAL XOTROTETO PUEBRITA\"])\n",
    "def text_to_clean_set(text, min_chars=7):\n",
    "    cleaned = \"\"\n",
    "    for char in text:\n",
    "        cleaned += char.upper() if char.isalpha() else \" \"\n",
    "    cleaned = cleaned.split()\n",
    "    return {word for word in cleaned if len(word)>=min_chars}\n",
    "\n",
    "anagramas = rdd_textos.flatMap(text_to_clean_set)\\\n",
    "    .map(lambda x: (\"\".join(sorted(x)), (x, set([x]))))\\\n",
    "    .reduceByKey(lambda x,y: (y[0], x[1].union(y[1])))\\\n",
    "    .map(lambda x: list(x[1][1]))\\\n",
    "    .filter(lambda x: len(x) > 1)\n",
    "\n",
    "anagramas.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "6- UBER almacena en un cluster todos los datos sobre el movimiento y viajes de todos sus vehículos. Existe un proceso que nos devuelve un RDD llamado trip_summary con los siguientes campos: (driver_id, car_id, trip_id, customer_id, date (YYYYMMDD), distance_traveled), Programar usando Py Spark un programa que nos indique cual fue el conductor con mayor promedio de distancia recorrida por viaje para Abril de 2016."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'31' fue el conductor con mayor promedio de distancia por viaje (15.59) para el mes solicitado\n"
     ]
    }
   ],
   "source": [
    "cant_viajes, cant_conductores, cant_clientes = 50000, 50, 2000\n",
    "random_date = lambda: fake.date_between(start_date='-5y').strftime('%Y%m%d')\n",
    "random_driver = lambda: random.choice(range(cant_conductores))\n",
    "random_car = lambda: random.choice(range(cant_conductores))\n",
    "random_customer = lambda: random.choice(range(cant_clientes))\n",
    "random_distance = lambda: round(random.random()*20+1, 1)\n",
    "\n",
    "trip_summary = sc.parallelize([(random_driver(), random_car(), viaje, random_customer(), random_date(), random_distance()) for viaje in range(cant_viajes)])\n",
    "\n",
    "def top_monthly_driver(trip_summary, month):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        trip_summary: pyspark.rdd [driver_id, car_id, trip_id, customer_id, date (YYYYMMDD), distance_traveled)]\n",
    "        month: YYYYMM\n",
    "    Returns:\n",
    "        driver_id, avg_distance\n",
    "    \"\"\"\n",
    "    driver_id, avg_distance = trip_summary.filter(lambda x: x[4][:-2]==month)\\\n",
    "        .map(lambda x: (x[0], (x[-1],1)))\\\n",
    "        .reduceByKey(lambda x,y: (x[0]+y[0],x[1]+y[1]))\\\n",
    "        .map(lambda x: (x[0], round(x[1][0]/x[1][1],2)))\\\n",
    "        .max(lambda x: x[1])\n",
    "    return driver_id, avg_distance\n",
    "\n",
    "driver_id, avg_distance = top_monthly_driver(trip_summary, \"201604\")\n",
    "print(\"'{}' fue el conductor con mayor promedio de distancia por viaje ({}) para el mes solicitado\".format(driver_id,avg_distance))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "8- Contamos con un cluster que tiene 4 computadoras. Queremos aprovechar el paralelismo del cluster para calcular los números primos entre 2 y 20.000.000. Para esto usaremos el conocido algoritmo de la criba de Eratóstenes. Por ejemplo si empezamos con una lista de tipo (2,3,4,5,6,7,8...) en un primer paso eliminamos todos los que son mayores a 2 y divisibles por 2 y nos queda (2,3,5,7,9,11,13...) luego eliminamos todos los mayores a 3 divisibles por 3 y nos queda (2,3,5,7,11,13....etc) luego todos los divisibles por 5 y así sucesivamente. El resultado final es una lista de números que son primos. Programar este programa usando PySpark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no hay caso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "9- Se cuenta con un RDD con información sobre patentamientos de autos con la siguiente información (patente, marca, modelo, versión, tipo_vehiculo, provincia, fecha), donde tipo_vehiculo indica si la unidad patentada es auto, pickup, camión o moto. Se pide generar un programa en pySpark que indique la marca y modelo del auto más patentado por tipo de vehículo en la provincia de Buenos Aires en el mes de Abril de 2017."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MOTO: FIAT ASTUTA con 17 patentamientos\n",
      "PICKUP: FORD ECOSPORT con 16 patentamientos\n",
      "CAMION: MERCEDES CLASE C con 15 patentamientos\n",
      "AUTO: FORD SOLO con 13 patentamientos\n"
     ]
    }
   ],
   "source": [
    "random_patente = lambda: fake.license_plate()\n",
    "provincias = [\"BUENOS AIRES\", \"MENDOZA\", \"MISIONES\", \"CORDOBA\", \"CHUBUT\"]\n",
    "random_provincia = lambda: random.choice(provincias)\n",
    "random_date = lambda: fake.date_between(start_date='-5y').strftime('%Y%m%d')\n",
    "vehiculos = {\n",
    "    \"FORD\": {\n",
    "        (\"AUTO\",\"FIESTA\"): [\"SEDAN\", \"CABRIOLET\"],\n",
    "        (\"AUTO\",\"SOLO\"): [\"SEDAN\", \"CABRIOLET\"],\n",
    "        (\"AUTO\",\"LEOPARDO\"): [\"SEDAN\", \"CABRIOLET\"],\n",
    "        (\"PICKUP\",\"ECOSPORT\"): [\"SEDAN\", \"CABRIOLET\"],\n",
    "    },\n",
    "    \"MERCEDES\": {\n",
    "        (\"PICKUP\",\"CLASE A\"): [\"2.0\",\"3.0\"],\n",
    "        (\"PICKUP\",\"CLASE B\"): [\"3.0\",\"4.0\"],\n",
    "        (\"CAMION\",\"CLASE C\"): [\"SEDAN\",\"COUPE\"],\n",
    "        (\"MOTO\",\"CLASE D\"): [\"2.0\",\"3.0\"],\n",
    "        (\"AUTO\",\"CLASE E\"): [\"3.0\",\"4.0\"],\n",
    "        (\"AUTO\",\"CLASE F\"): [\"SEDAN\",\"COUPE\"],\n",
    "    },\n",
    "    \"FIAT\": {\n",
    "        (\"MOTO\",\"SIENA\"): [\"MINI\", \"POWER\"],\n",
    "        (\"PICKUP\",\"PUNTO\"): [\"CABRIO\", \"CLASSIC\"],\n",
    "        (\"MOTO\",\"ASTUTA\"): [\"MINI\", \"POWER\"],\n",
    "        (\"CAMION\",\"BRAVO\"): [\"CABRIO\", \"CLASSIC\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "def random_vehiculo(vehiculos):\n",
    "    marca = random.choice(list(vehiculos.keys()))\n",
    "    tipo, modelo = random.choice(list(vehiculos[marca].keys()))\n",
    "    version = random.choice(vehiculos[marca][(tipo,modelo)])\n",
    "    return (marca,modelo,version,tipo)\n",
    "\n",
    "cant_patentamientos = 50000\n",
    "rdd_patentamientos = sc.parallelize([(random_patente(),*random_vehiculo(vehiculos),random_provincia(),random_date()) for x in range(cant_patentamientos)])\n",
    "\n",
    "def mas_patentados(patentamientos, provincia, mes):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        rdd_patentamientos: pyspark.RDD [(patente, marca, modelo, versión, tipo_vehiculo, provincia, fecha<AAAAMMDD>)]\n",
    "        provincia: str\n",
    "        mes: str<AAAAMM>\n",
    "    Returns:\n",
    "        [(tipo_vehiculo, (marca, modelo, patentamientos))]\n",
    "    \"\"\"\n",
    "    solucion = rdd_patentamientos.filter(lambda x: x[-2]==provincia and x[-1][:-2]==mes)\\\n",
    "        .map(lambda x: ((x[4],x[1],x[2]), 1))\\\n",
    "        .reduceByKey(lambda x,y: (x+y))\\\n",
    "        .map(lambda x: (x[0][0], (x[0][1],x[0][2],x[1])))\\\n",
    "        .reduceByKey(lambda x,y: x if x[-1]>y[-1] else y)\\\n",
    "        .collect()\n",
    "    return solucion\n",
    "\n",
    "for tipo, (marca, modelo, patentamientos) in mas_patentados(rdd_patentamientos, \"BUENOS AIRES\", \"201704\"):\n",
    "    print(\"{}: {} {} con {} patentamientos\".format(tipo,marca,modelo,patentamientos))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
